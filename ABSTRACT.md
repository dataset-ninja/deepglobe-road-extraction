**DeepGlobe 2018 Road Extraction** dataset is a part of the DeepGlobe 2018 Satellite Image Understanding Challenge, which includes three public competitions for segmentation, detection, and classification tasks on satellite images:

1. <i>Road Extraction Challenge (current)</i>: This challenge focuses on automating the extraction of road and street networks from satellite images, particularly in disaster zones and developing countries. The aim is to facilitate automated crisis response and expand map coverage for improved connectivity.
2. <i>Building Detection Challenge</i>: With a focus on modeling urban demographics for disaster response and recovery, this challenge involves the automatic detection of buildings from satellite images. This facilitates the remote gathering of urban information and spatial distribution details of urban settlements.
3. <i>Land Cover Classification Challenge [(available on DatasetNinja)](https://datasetninja.com/deepglobe)</i>: This challenge revolves around the automatic classification and segmentation of land cover types from satellite images. It is crucial for sustainable development, agriculture, forestry, and urban planning.

There have been several datasets proposed in the literature for benchmarking algorithms for semantic segmentation of overhead imagery. Some of these can be enumerated as the [TorontoCity](https://arxiv.org/abs/1612.00423) dataset, the [ISPRS 2D semantic labeling dataset](http://www2.isprs.org/commissions/comm3/wg4/semantic-labeling.html), [the Mnih dataset](https://www.cs.toronto.edu/~vmnih/docs/Mnih_Volodymyr_PhD_Thesis.pdf), the [SpaceNet dataset](https://medium.com/thedownlinq/introducing-the-spacenet-roaddetection-and-routing-challenge-anddataset-7604de39b779.
) and the [ISPRS Benchmark for Multi-Platform Photogrammetry](http://www2.isprs.org/commissions/comm1/icwg15b/benchmark_main.html). The satellite imagery used in DeepGlobe for the road extraction challenge is sampled from the DigitalGlobe + [Vivid Images dataset](https://dg-cms-uploads-production.s3.amazonaws.com/uploads/document/file/2/DG_Basemap_Vivid_DS_1.pdf). It covers images captured over Thailand, Indonesia, and India. The ground resolution of the image pixels is 50 cm/pixel. The images consist of 3 channels (Red, Green, and Blue). Each of the original GeoTIFF images has a resolution of 19584×19584 pixels. The annotation process starts by tiling and loading these images in [QGIS](https://qgis.org/en/site/). Based on this tiling, authors determine useful areas to sample from those countries. For designating useful areas, authors sample data uniformly between rural and urban areas. After sampling authors select the corresponding DigitalGlobe tiff images belonging to those areas. These images are then cropped to extract useful subregions and relevant subregions are sampled by GIS experts. (A useful subregion denotes a part of the image where authors have a good relative ratio between positive and negative examples.) Also, while selecting these subregions, authors try to sample interesting areas uniformly, e.g., those with different types of road surfaces (unpaved, paved, dirt roads), rural and urban areas, etc. It is important to note that the labels generated are pixel-based, where all pixels belonging to the road are labeled, instead of labeling only the centerline.

The final road dataset consists of a total of 8570 images and spans a total land area of 2220 km². Of those, 6226 images (72.7% of the dataset), spanning a total of 1632 km², were split as the *training* dataset. 1243 images,spanning 362 km², were chosen as the *validation* dataset and 1101 images were chosen for *testing* which cover a total land area of 288 km². The split of the dataset into training/validation/testing subsets is conducted by randomizing among tiles to aim for an approximate distribution of 70%/15%/15%. The training dataset consists of 4.5% positive and 95.5% negative pixels, the validation dataset consists of 3% positive and 97% negative pixels and the test dataset consists of 4.1% positive and 95.9% negative pixels. 
