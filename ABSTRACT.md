There have been several datasets proposed in the literature for benchmarking algorithms for semantic segmentation of overhead imagery. Some of these can be enumerated as the [TorontoCity](https://arxiv.org/abs/1612.00423) dataset, the [ISPRS 2D semantic labeling dataset](http://www2.isprs.org/commissions/comm3/wg4/semantic-labeling.html), [the Mnih dataset](https://www.cs.toronto.edu/~vmnih/docs/Mnih_Volodymyr_PhD_Thesis.pdf), the [SpaceNet dataset](https://medium.com/thedownlinq/introducing-the-spacenet-roaddetection-and-routing-challenge-anddataset-7604de39b779.
) and the [ISPRS Benchmark for Multi-Platform Photogrammetry](http://www2.isprs.org/commissions/comm1/icwg15b/benchmark_main.html). The satellite imagery used in DeepGlobe for the road extraction challenge is sampled from the DigitalGlobe + [Vivid Images dataset](https://dg-cms-uploads-production.s3.amazonaws.com/uploads/document/file/2/DG_Basemap_Vivid_DS_1.pdf). It covers images captured over Thailand, Indonesia, and India. The ground resolution of the image pixels is 50 cm/pixel. The images consist of 3 channels (Red, Green and Blue). Each of the original geotiff images are 19584Ã—19584 pixels. The annotation process starts by tiling and loading these images in [QGIS](https://qgis.org/en/site/). Based on this tiling, authors determine useful areas to sample from those countries. For designating useful areas, authors sample data uniformly between rural and urban areas. After sampling authors select the corresponding DigitalGlobe tiff images belonging to those areas. These images are then cropped to extract useful subregions and relevant subregions are sampled by GIS experts. (A useful subregion denotes a part of the image where authors have a good relative ratio between positive and negative examples.) Also, while selecting these subregions, authors try to sample interesting areas uniformly, e.g., those with different types of road surfaces (unpaved, paved, dirt roads), rural and urban areas, etc. An example of one image crop is illustrated in the left panel of Figure 1. It is important to note that the labels generated are pixel-based, where all pixels belonging to the road are labeled, instead of labeling only the centerline.
The final road dataset consists of a total of 8570 images and spans a total land area of 2220$km^2$. Of those, 6226 images (72.7% of the dataset), spanning a total of 1632$km^2$, were split as the *training* dataset. 1243 images,spanning 362$km^2$, were chosen as the *validation* dataset and 1101 images were chosen for *testing* which cover a total land area of 288$km^2$. The split of the dataset to training/validation/testing subsets is conducted by randomizing among tiles to aim for an approximate distribution of 70%/15%/15%. The training dataset consists of 4.5% positive and 95.5% negative pixels, the validation dataset consists of 3% positive and 97% negative pixels and the test dataset consists of 4.1% positive and 95.9% negative pixels. 

